---
title: "Laboratorio 1"
author: "Joshua Cervantes"
date: "`r Sys.Date()`"
theme: Cerulean
format:
    html:
        self-contained: true
        embed-resources: true
execute:
    echo: true
    warning: false
    error: false
    freeze: auto
    python: python3
---


```{python}
# Se procede a importar los paquetes
import pandas as pd
import numpy as np
from cod.ejercicio import ejercicio_1 as ej1
import seaborn as sns
import matplotlib.pyplot as plt
```


# Ejercicio 1

## Notas escolares

```{python}
df_notas = pd.read_csv("./data/NotasEscolares.csv")

```



Se procede a llamar la clase que programamos 
```{python}
notas_escolares_kmeans = ej1(df_notas.drop(["Estudiante"], axis = 1))

```

### 3 cluster
Hacemos el multistart con 3 clases
```{python}

notas_ms_3 = notas_escolares_kmeans.kmeans_multistart(3, 1000)

```


```{python}
notas_ms_3["mejor_inercia"]
```


La mejor inercia es inferior a 9. 

```{python}
notas_ms_3["tau"]
```

En cuanto a la tasa de atracción en este caso observamos que inferior al 15. 


```{python}
notas_escolares_kmeans.plot_comportamiento_tiempos()
```

Los tiempos se encuentran realmente bajos siendo inferiores al segundo. 


```{python}
notas_escolares_kmeans.plot_comportamiento_locales()
```


Se puede observar que la inercia en la mayoría de los casos se ubica poder debajo de 10. Aunque hay casos en los que la inercia queda realmente alta. 



```{python}
notas_ms_3["iteraciones_promedio"]

```


Se puede obsevar que en esta tabla de datos cada multistart en promedio converge rápidamente.



### 4 cluster

Hacemos el multistart con 4 clases
```{python}

notas_ms_4 = notas_escolares_kmeans.kmeans_multistart(4, 1000)

```


```{python}
notas_ms_4["mejor_inercia"]
```

Se puede observar que la inercia obtenida en este caso es inferior al caso anterio como es de esperar dado que a mayor clases la inercia intraclase disminuye. 

```{python}
notas_ms_4["tau"]
```
La tasa de atracción en este caso es mucho menor que en el caso anterior esto dado a que existe una mayor cantidad de combinaciones que se pueden hacer. 



```{python}
notas_escolares_kmeans.plot_comportamiento_tiempos()
```


Los tiempos son mucho menores, esto está relacionado con que es más fácil encontrar un óptimo al permitirse una mayor cantidad de casos. 

```{python}
notas_escolares_kmeans.plot_comportamiento_locales()
```

La inercia se comporta muy similar al caso anterior siendo inferior a 10, pero sí se nota que es mucho menor en general. 

```{python}
notas_ms_4["iteraciones_promedio"]

```

Se disminuye el número de iteraciones.


## Peces de Amiard

```{python}
df_peces = pd.read_excel("./data/Amiard.xls", skiprows=1)
```


```{python}
peces_kmeans = ej1(df_peces.drop(["Pez"], axis = 1))

```

### 3 cluster

Hacemos el multistart con 3 clases
```{python}

peces_ms_3 = peces_kmeans.kmeans_multistart(3, 1000)

```


```{python}
peces_ms_3["mejor_inercia"]
```


```{python}
peces_ms_3["tau"]
```




```{python}
peces_kmeans.plot_comportamiento_tiempos()
```

En este caso se puede observar que los tiempos varian más que en el ejemplo de las notas, esto está relacionado con que hay una mayor cantidad de individuos lo que genera mayor tiempo de ejecución y además afecta el rendimiento del procesador donde pueden haber ejecuciones más prolongadas. 

```{python}
peces_kmeans.plot_comportamiento_locales()
```

En este caso también se ubica una mayor variabilidad en la distirbución de las inercias. Se observa que existen claramente valores los cuales se repiten más alrededor de 180 y 190.


```{python}
peces_ms_3["iteraciones_promedio"]

```

Aquí en comparación con el ejercicio anterior existen más iteraciones en promedio. 


### 4 cluster

Hacemos el multistart con 4 clases
```{python}

peces_ms_4 = peces_kmeans.kmeans_multistart(4, 1000)

```


```{python}
peces_ms_4["mejor_inercia"]
```

La inercia es mejor en este caso como es de esperarse. 

```{python}
peces_ms_4["tau"]
```


La tasa de atracción en este caso es mucho menor dado que existe una mayor cantidad de combinaciones. 

```{python}
peces_kmeans.plot_comportamiento_tiempos()
```


El tiempo de ejecución presenta una mayor cantidad de valores superiores a los mostrados en el ejemplo anterior en general. 

```{python}
peces_kmeans.plot_comportamiento_locales()
```



```{python}
peces_ms_4["iteraciones_promedio"]

```

En este caso existe una menor cantidad de iteraciones. 

## Iris

```{python}
from sklearn import datasets

iris = datasets.load_iris()
iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])
```




```{python}
iris_kmeans = ej1(iris.drop(["target"], axis = 1))

```

### 3 cluster

Hacemos el multistart con 3 clases
```{python}

iris_ms_3 = iris_kmeans.kmeans_multistart(3, 1000)

```


```{python}
iris_ms_3["mejor_inercia"]
```


```{python}
iris_ms_3["tau"]
```




```{python}
iris_kmeans.plot_comportamiento_tiempos()
```


```{python}
iris_kmeans.plot_comportamiento_locales()
```


Se puede observar que la mayoría de los óptimo se ubican alrededor de 140. 


```{python}
iris_ms_3["iteraciones_promedio"]

```




### 4 cluster

Hacemos el multistart con 4 clases
```{python}

iris_ms_4 = iris_kmeans.kmeans_multistart(4, 1000)

```


```{python}
iris_ms_4["mejor_inercia"]
```

La inercia mejora como es de esperarse.

```{python}
iris_ms_4["tau"]
```

Vemos que en este caso la tasa de atracción disminuye.




```{python}
iris_kmeans.plot_comportamiento_tiempos()
```


```{python}
iris_kmeans.plot_comportamiento_locales()
```



```{python}
iris_ms_4["iteraciones_promedio"]

```

En este caso el número de iteraciones aumenta.

# Ejercicio 2

## Mobile

```{r}

library(tidyverse)
source("./cod/ejercicio2.R")
df.mobile <- data.table::fread("./data/MOBILE.txt", sep = " ")

```


```{r}
#| eval: false
objeto.mobile <- fn.ejercicio.2(datos = df.mobile, control = list(maxit = 150))

save(file = "./res/mobile.RData", objeto.mobile)

```


```{r}
load(file = "./res/mobile.RData")
```


Mejor 1-stress obtenido

```{r}
objeto.mobile$mejor.resultado$value
```

Se presenta la tasa de atracción $\tau$
```{r}
objeto.mobile$tau*100
```


En este caso se observa que da 1% esto quiere decir que el mejor resultado se obtuvo una sola vez. Se intentó hacer estimar un error porcentual respecto al mejor resultado y el máximo de las diferencias de los valores, pero de esta manera igualmente se siguió obteniendo 1% como tasa de atracción.

Se presenta el $\overline{\sigma(\mathbf{X})}$ 

```{r}
objeto.mobile$promedio.stress
```

Se reporta el tiempo de los multistart


```{r}
ggplot2::ggplot(data.frame("tiempos" = objeto.mobile$tiempos), aes(x = tiempos)) +
        ggplot2::geom_histogram() +
        ggplot2::labs(title = "Distribucion de los tiempos de los multistart", x = "Tiempo (s)", y = "Frecuencia") +
        ggplot2::theme_minimal()

```


Se presenta el promedio de iteraciones de cada multistart
```{r}
objeto.mobile$promedio.iteraciones
```

En este caso se ubica en 150, pero esto se debe a que se había colocado como máximo 150 iteraciones, esto lo que puede indicar es que el modelo no está convergiendo por lo que se pueden requerir más iteraciones. Lamentablemente se ejecutó dos veces y en ambos casos se alcanzó como promedio el máximo de las iteracion 100 y 150. 

Se presenta un histograma para mostrar los diferentes valores óptimos que se pueden tener.

```{r}
ggplot2::ggplot(data.frame("stress" = objeto.mobile$stress), aes(x = stress)) +
        ggplot2::geom_histogram() +
        ggplot2::labs(title = "Distribucion de los optimos locales", x = "Stress", y = "Frecuencia") +
        ggplot2::theme_minimal()
    

```


En este caso se puede observar que los multistart en este caso se ubican al rededor de 7.5 segundo y varian en $\pm 0.5$ segundo. Se tiene una distribución en cierta medida simétrica.

```{r}
ggplot2::ggplot(objeto.mobile$df.valores, aes(x = x, y = y, color = Etiquetas)) +
        ggplot2::geom_point() +
        ggplot2::labs(title = "Coordenadas optimas", x = "X", y = "Y") +
        ggplot2::theme_minimal()
```

En el gráfico no se ubica una gran diferencia entre los puntos, pero se puede observar que sí existe variación de acuerdo a los colores. 

## Colas
```{r}
#Se procede a cargar la informacion de 
df.colas <- data.table::fread("./data/COLAS.txt", sep = " ")
df.etiquetas <- data.table::fread("./data/colas.eti.txt", sep = " ")[2:11]
```

```{r}
colnames(df.colas) <- df.etiquetas$V1
df.colas$rows <- df.etiquetas$V1

df.colas <- df.colas %>% 
    pivot_longer(cols = Diet_Pepsi:Diet_Rite, names_to = "V2", values_to = "V3")

colnames(df.colas) <- paste("V", 1:3, sep = "")
```


```{r}
#| eval: false
objeto.colas <- fn.ejercicio.2(datos = df.colas)
save(file = "./res/colas.RData", objeto.colas)
```


```{r}
load(file = "./res/colas.RData")
```


Mejor 1-stress obtenido

```{r}
objeto.colas$mejor.resultado$value
```

Se presenta la tasa de atracción $\tau$
```{r}
objeto.colas$tau*100
```


En este caso también se observa que la atracción es inferior a 1%. 

Se presenta el $\overline{\sigma(\mathbf{X})}$ 

```{r}
objeto.colas$promedio.stress
```

El promedio es notoriamente superior al mejor valor obtenido de 1-stress.

Se reporta el tiempo de los multistart
```{r}
objeto.colas$plot.time
```

Los tiempos en este caso sí son asimétricos. Al igual que los óptimos como se ve más adelante. 

Se presenta el promedio de iteraciones de cada multistart
```{r}
objeto.colas$promedio.iteraciones
```


En este caso se puede observar que el promedio de iteraciones sí es inferior al máximo permitido por lo que el algoritmo sí parece converger. 

Se presenta un histograma para mostrar los diferentes óptimos obtenidos
```{r}
objeto.colas$plot.optimos
```


```{r}
objeto.colas$plot.coordendas.optimas
```

En este caso es interesante que se forma un comportamiento de rendondo, esto está relacionado con la norma que se está usando y se puede observar que una misma cola puede tener diferentes coordernadas mostrando las múltiples soluciones que existen. 

## Ciudades Costa Rica

```{r}
df.ciudades <- openxlsx::read.xlsx("./data/Distancia_ciudades.xlsx")
df.ciudades <- df.ciudades %>% 
    pivot_longer(cols = Alajuela:Puntarenas, names_to = "V2", values_to = "V3")


colnames(df.ciudades) <- paste("V", 1:3, sep = "")
```

```{r}
#| eval: false
objeto.ciudades <- fn.ejercicio.2(datos = df.ciudades)
save(file = "./res/ciudades.RData", objeto.ciudades)
```



```{r}
load(file = "./res/ciudades.RData")
```


Mejor 1-stress obtenido

```{r}
objeto.ciudades$mejor.resultado$value
```

Se presenta la tasa de atracción $\tau$
```{r}
objeto.ciudades$tau*100
```

En este caso se observa que da 1% esto quiere decir que el mejor resultado se obtuvo una sola vez con la mismca combinación de coordenadas.

Se presenta el $\overline{\sigma(\mathbf{X})}$ 

```{r}
objeto.ciudades$promedio.stress
```

Se reporta el tiempo de los multistart
```{r}
objeto.ciudades$plot.time
```


Se presenta el promedio de iteraciones de cada multistart
```{r}
objeto.ciudades$promedio.iteraciones
```

En este caso se puede observar que el promedio de iteraciones sí es inferior al máximo permitido por lo que el algoritmo sí parece converger. 


Se presenta un histograma para mostrar los diferentes valores óptimos que se pueden tener.

```{r}
objeto.ciudades$plot.optimos
```

Se puede observar que la mayoría de los óptimos tienen un valor similar.

```{r}
objeto.ciudades$plot.coordendas.optimas
```

Se muestra un comportamiento similar al de las colas y se refuerza el hecho de que pueden existir múltiples soluciones para una misma ciudad.